{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "JQT7UkQQ7MMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "sgr3DASQ2zL7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AND Gate"
      ],
      "metadata": {
        "id": "EtQAGYNb3qOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Truth table of AND gate\n",
        "    \n",
        "    Input 1\t| Input 2\t | Output\n",
        "    0\t      |   0         |   0\n",
        "    0\t      |   1         |   0\n",
        "    1\t      |   0         |   0\n",
        "    1\t      |   1         |   1"
      ],
      "metadata": {
        "id": "WXw6-Ypj1pyq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-JDATJAY0M0b"
      },
      "outputs": [],
      "source": [
        "def step_function(x):\n",
        "    return np.where(x >= 0, 1, 0)\n",
        "\n",
        "# AND Gate dataset (Truth table)\n",
        "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "outputs = np.array([[0], [0], [0], [1]])\n",
        "\n",
        "# Function to train the perceptron\n",
        "def train_perceptron(weights, bias, learning_rate=0.1, epochs=10000):\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass: Linear combination (weighted sum) + bias\n",
        "        linear_output = np.dot(inputs, weights) + bias\n",
        "\n",
        "        # Apply activation function (step function)\n",
        "        prediction = step_function(linear_output)\n",
        "\n",
        "        # Calculate the error\n",
        "        error = outputs - prediction\n",
        "\n",
        "        # Update weights and bias using gradient descent\n",
        "        weights += learning_rate * np.dot(inputs.T, error)\n",
        "        bias += learning_rate * np.sum(error)\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "def test_perceptron(weights, bias):\n",
        "    # Testing the trained model\n",
        "    test_output = step_function(np.dot(inputs, weights) + bias)\n",
        "    return test_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Weights and Bias Initialization\n",
        "weights_random = np.random.rand(2, 1)   # Random weights\n",
        "bias_random = np.random.rand(1)         # Random bias\n",
        "\n",
        "print(\"Training with Random Weights and Bias\")\n",
        "# Training the perceptron with random weights\n",
        "trained_weights_random, trained_bias_random = train_perceptron(weights_random, bias_random)\n",
        "\n",
        "# Testing the perceptron with random weights\n",
        "test_output_random = test_perceptron(trained_weights_random, trained_bias_random)\n",
        "\n",
        "print(\"\\nRandom Weights Training Results:\")\n",
        "print(\"\\nFinal Weights: \", trained_weights_random)\n",
        "print(\"\\nFinal Bias: \", trained_bias_random)\n",
        "print(\"\\nOutput: \", test_output_random)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEiJt1372_B8",
        "outputId": "14f2e805-c6e3-4747-debd-a893a62f6e8b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Random Weights and Bias\n",
            "\n",
            "Random Weights Training Results:\n",
            "\n",
            "Final Weights:  [[0.32261932]\n",
            " [0.37941079]]\n",
            "\n",
            "Final Bias:  [-0.54759046]\n",
            "\n",
            "Output:  [[0]\n",
            " [0]\n",
            " [0]\n",
            " [1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defined Weights and Bias Initialization\n",
        "weights_defined = np.array([[0.5], [0.5]])  # Defined weights\n",
        "bias_defined = np.array([0.5])              # Defined bias\n",
        "\n",
        "print(\"\\nTraining with Defined Weights and Bias\")\n",
        "# Training the perceptron with defined weights\n",
        "trained_weights_defined, trained_bias_defined = train_perceptron(weights_defined, bias_defined)\n",
        "\n",
        "# Testing the perceptron with defined weights\n",
        "test_output_defined = test_perceptron(trained_weights_defined, trained_bias_defined)\n",
        "\n",
        "print(\"\\nDefined Weights Training Results:\")\n",
        "print(\"\\nFinal Weights: \", trained_weights_defined)\n",
        "print(\"\\nFinal Bias: \", trained_bias_defined)\n",
        "print(\"\\nOutput: \", test_output_defined)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLAQE9o_2_j3",
        "outputId": "02944b71-a5cb-4962-8f08-2e5facec5aeb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with Defined Weights and Bias\n",
            "\n",
            "Defined Weights Training Results:\n",
            "\n",
            "Final Weights:  [[0.2]\n",
            " [0.2]]\n",
            "\n",
            "Final Bias:  [-0.3]\n",
            "\n",
            "Output:  [[0]\n",
            " [0]\n",
            " [0]\n",
            " [1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### How do the weights and bias values change during training for the AND gate?\n",
        "\n",
        "  - Weights: Adjust based on the input and error. They increase when both inputs are 1 to reinforce the correct prediction of 1 and decrease for incorrect predictions.\n",
        "  - Bias: Shifts based on the error to adjust the decision boundary, ensuring correct classification.\n",
        "  - Over time, weights and bias converge to values that minimize the prediction error for all inputs.\n",
        "\n",
        "### Can the perceptron successfully learn the AND logic with a linear decision boundary?\n",
        "\n",
        "  Yes, the AND gate is linearly separable, so a perceptron can learn a linear decision boundary that correctly separates (1, 1) (output 1) from other inputs (0, 0), (0, 1), (1, 0) (output 0)."
      ],
      "metadata": {
        "id": "gWYwVS6v4Tfx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OR Gate"
      ],
      "metadata": {
        "id": "hO-b0IO23uOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Truth Table of OR Gate\n",
        "\n",
        "    Input 1\t| Input 2\t | Output\n",
        "    0\t      |   0         |   0\n",
        "    0\t      |   1         |   1\n",
        "    1\t      |   0         |   1\n",
        "    1\t      |   1         |   1"
      ],
      "metadata": {
        "id": "fQHrCqsj5R6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step function (activation function)\n",
        "def step_function(x):\n",
        "    return np.where(x >= 0, 1, 0)\n",
        "\n",
        "# OR Gate dataset (Truth table)\n",
        "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "outputs = np.array([[0], [1], [1], [1]])\n",
        "\n",
        "# Function to train the perceptron and monitor progress\n",
        "def train_perceptron(weights, bias, learning_rate=0.1, epochs=10000, print_interval=1000):\n",
        "    for epoch in range(epochs):\n",
        "        # Linear combination (weighted sum)\n",
        "        linear_output = np.dot(inputs, weights) + bias\n",
        "\n",
        "        # Apply step function\n",
        "        prediction = step_function(linear_output)\n",
        "\n",
        "        # Calculate the error\n",
        "        error = outputs - prediction\n",
        "\n",
        "        # Update weights and bias\n",
        "        weights += learning_rate * np.dot(inputs.T, error)\n",
        "        bias += learning_rate * np.sum(error)\n",
        "\n",
        "        # Print progress at specified intervals\n",
        "        if (epoch + 1) % print_interval == 0 or epoch == 0:\n",
        "            print(f\"Epoch {epoch + 1}: Weights = {weights.T}, Bias = {bias}, Error = {error.T}\")\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "# Function to test the trained perceptron\n",
        "def test_perceptron(weights, bias):\n",
        "    # Testing the perceptron\n",
        "    test_output = step_function(np.dot(inputs, weights) + bias)\n",
        "    return test_output\n",
        "\n",
        "# Initialize weights and bias randomly\n",
        "weights_random = np.random.rand(2, 1)  # Random weights\n",
        "bias_random = np.random.rand(1)        # Random bias\n",
        "\n",
        "print(\"Training with Random Weights and Bias\")\n",
        "# Training the perceptron and print progress\n",
        "trained_weights, trained_bias = train_perceptron(weights_random, bias_random)\n",
        "\n",
        "# Testing the perceptron\n",
        "test_output = test_perceptron(trained_weights, trained_bias)\n",
        "\n",
        "print(\"\\nFinal Weights: \", trained_weights)\n",
        "print(\"\\nFinal Bias: \", trained_bias)\n",
        "print(\"\\nOutput:\", test_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDC8NOVH3wKE",
        "outputId": "3a2508cb-f64e-4fca-836d-93125019d5c6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Random Weights and Bias\n",
            "Epoch 1: Weights = [[0.6436158  0.44839251]], Bias = [0.10474454], Error = [[-1  0  0  0]]\n",
            "Epoch 1000: Weights = [[0.6436158  0.44839251]], Bias = [-0.09525546], Error = [[0 0 0 0]]\n",
            "Epoch 2000: Weights = [[0.6436158  0.44839251]], Bias = [-0.09525546], Error = [[0 0 0 0]]\n",
            "Epoch 3000: Weights = [[0.6436158  0.44839251]], Bias = [-0.09525546], Error = [[0 0 0 0]]\n",
            "Epoch 4000: Weights = [[0.6436158  0.44839251]], Bias = [-0.09525546], Error = [[0 0 0 0]]\n",
            "Epoch 5000: Weights = [[0.6436158  0.44839251]], Bias = [-0.09525546], Error = [[0 0 0 0]]\n",
            "Epoch 6000: Weights = [[0.6436158  0.44839251]], Bias = [-0.09525546], Error = [[0 0 0 0]]\n",
            "Epoch 7000: Weights = [[0.6436158  0.44839251]], Bias = [-0.09525546], Error = [[0 0 0 0]]\n",
            "Epoch 8000: Weights = [[0.6436158  0.44839251]], Bias = [-0.09525546], Error = [[0 0 0 0]]\n",
            "Epoch 9000: Weights = [[0.6436158  0.44839251]], Bias = [-0.09525546], Error = [[0 0 0 0]]\n",
            "Epoch 10000: Weights = [[0.6436158  0.44839251]], Bias = [-0.09525546], Error = [[0 0 0 0]]\n",
            "\n",
            "Final Weights:  [[0.6436158 ]\n",
            " [0.44839251]]\n",
            "\n",
            "Final Bias:  [-0.09525546]\n",
            "\n",
            "Output: [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### What changes in the perceptron's weights are necessary to represent the OR gate logic?\n",
        "\n",
        "  The perceptron's weights should increase when either of the inputs is 1, to correctly predict the output 1. Initially random weights will adjust so that they favor any input combination where at least one of the inputs is 1.\n",
        "  The final weights will be positive values that linearly combine the inputs, ensuring that the decision boundary separates the input (0, 0) (output 0) from the other inputs (0, 1), (1, 0), and (1, 1) (output 1).\n",
        "\n",
        "### How does the linear decision boundary look for the OR gate classification?\n",
        "\n",
        "  The linear decision boundary for the OR gate will be such that the point (0, 0) is on one side of the boundary (output 0), while the points (0, 1), (1, 0), and (1, 1) are on the other side (output 1).\n",
        "  The boundary will be a straight line in the input space, effectively separating the 0 output from the 1 outputs, demonstrating that the OR gate is linearly separable."
      ],
      "metadata": {
        "id": "61YzTZXo43o2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AND-NOT Gate"
      ],
      "metadata": {
        "id": "ROIJw_t96Swc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Truth Table of AND-NOT Gate\n",
        "\n",
        "    Input 1\t| Input 2\t | Output\n",
        "    0\t      |   0         |   0\n",
        "    0\t      |   1         |   0\n",
        "    1\t      |   0         |   1\n",
        "    1\t      |   1         |   0"
      ],
      "metadata": {
        "id": "mcME_v2u6jAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step function (activation function)\n",
        "def step_function(x):\n",
        "    return np.where(x >= 0, 1, 0)\n",
        "\n",
        "# AND-NOT Gate dataset (Truth table)\n",
        "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "outputs = np.array([[0], [0], [1], [0]])\n",
        "\n",
        "# Function to train the perceptron\n",
        "def train_perceptron(weights, bias, learning_rate=0.1, epochs=10000):\n",
        "    for epoch in range(epochs):\n",
        "        # Linear combination (weighted sum)\n",
        "        linear_output = np.dot(inputs, weights) + bias\n",
        "\n",
        "        # Apply step function\n",
        "        prediction = step_function(linear_output)\n",
        "\n",
        "        # Calculate the error\n",
        "        error = outputs - prediction\n",
        "\n",
        "        # Update weights and bias\n",
        "        weights += learning_rate * np.dot(inputs.T, error)\n",
        "        bias += learning_rate * np.sum(error)\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "# Function to test the trained perceptron\n",
        "def test_perceptron(weights, bias):\n",
        "    # Test the perceptron\n",
        "    test_output = step_function(np.dot(inputs, weights) + bias)\n",
        "    return test_output\n",
        "\n",
        "# Initializing weights and bias randomly\n",
        "weights_random = np.random.rand(2, 1)  # Random weights\n",
        "bias_random = np.random.rand(1)        # Random bias\n",
        "\n",
        "print(\"Training with Random Weights and Bias\")\n",
        "# Training the perceptron\n",
        "trained_weights, trained_bias = train_perceptron(weights_random, bias_random)\n",
        "\n",
        "# Testing the perceptron\n",
        "test_output = test_perceptron(trained_weights, trained_bias)\n",
        "\n",
        "print(\"\\n Final Weights:\\n\", trained_weights)\n",
        "print(\"\\n Final Bias:\\n\", trained_bias)\n",
        "print(\"\\n Output: \", test_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_Ws7bIi4-tE",
        "outputId": "ec8ca38d-67fc-46ec-837b-a4b5ab3a594a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Random Weights and Bias\n",
            "\n",
            " Final Weights:\n",
            " [[ 0.25066603]\n",
            " [-0.19684618]]\n",
            "\n",
            " Final Bias:\n",
            " [-0.07184954]\n",
            "\n",
            " Output:  [[0]\n",
            " [0]\n",
            " [1]\n",
            " [0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### What is the perceptron's weight configuration after training for the AND-NOT gate?\n",
        "\n",
        "  After training, the weights will have adjusted to recognize that the output should be 1 when the first input is 1 and the second input is 0. The weights will ensure that when the first input is 1 and the second is 0, the output becomes positive and triggers the step function to output 1.\n",
        "\n",
        "### How does the perceptron handle cases where both inputs are 1 or 0?\n",
        "\n",
        "  When both inputs are 1 or both inputs are 0, the perceptron should output 0 because the linear combination of weights and bias won't exceed the step function's threshold in these cases. The perceptron learns to separate the input (1, 0) (output 1) from the others."
      ],
      "metadata": {
        "id": "5EgUO5Ob6zrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XOR Gate"
      ],
      "metadata": {
        "id": "fje1ufEe7wKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Truth Table of XOR Gate\n",
        "\n",
        "    Input 1\t| Input 2\t | Output\n",
        "    0\t      |   0         |   0\n",
        "    0\t      |   1         |   1\n",
        "    1\t      |   0         |   1\n",
        "    1\t      |   1         |   0"
      ],
      "metadata": {
        "id": "-7rt4QwC-SHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step function (activation function)\n",
        "def step_function(x):\n",
        "    return np.where(x >= 0, 1, 0)\n",
        "\n",
        "# XOR Gate dataset (Truth table)\n",
        "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "outputs = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Function to train the perceptron\n",
        "def train_perceptron(weights, bias, learning_rate=0.1, epochs=10000):\n",
        "    for epoch in range(epochs):\n",
        "        # Linear combination (weighted sum)\n",
        "        linear_output = np.dot(inputs, weights) + bias\n",
        "\n",
        "        # Apply step function\n",
        "        prediction = step_function(linear_output)\n",
        "\n",
        "        # Calculate the error\n",
        "        error = outputs - prediction\n",
        "\n",
        "        # Update weights and bias\n",
        "        weights += learning_rate * np.dot(inputs.T, error)\n",
        "        bias += learning_rate * np.sum(error)\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "# Function to test the trained perceptron\n",
        "def test_perceptron(weights, bias):\n",
        "    # Test the perceptron\n",
        "    test_output = step_function(np.dot(inputs, weights) + bias)\n",
        "    return test_output"
      ],
      "metadata": {
        "id": "cyyjQvAg7xeV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing weights and bias randomly\n",
        "weights_random = np.random.rand(2, 1)  # Random weights\n",
        "bias_random = np.random.rand(1)        # Random bias\n",
        "\n",
        "print(\"Training with Random Weights and Bias\")\n",
        "# Training the perceptron\n",
        "trained_weights, trained_bias = train_perceptron(weights_random, bias_random)\n",
        "\n",
        "# Testing the perceptron\n",
        "test_output = test_perceptron(trained_weights, trained_bias)\n",
        "\n",
        "print(\"\\nFinal Weights:\\n\", trained_weights)\n",
        "print(\"\\nFinal Bias:\\n\", trained_bias)\n",
        "print(\"\\nOutput:\", test_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFwlnOHW9uD4",
        "outputId": "c496fa81-cbe4-40c8-9b5f-eb02d51e0ff9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Random Weights and Bias\n",
            "\n",
            "Final Weights:\n",
            " [[ 0.06969919]\n",
            " [-0.06229789]]\n",
            "\n",
            "Final Bias:\n",
            " [-0.09412006]\n",
            "\n",
            "Output: [[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Why does the Single Layer Perceptron struggle to classify the XOR gate?\n",
        "\n",
        "  The SLP can only learn to separate inputs that are linearly separable. In the case of XOR, the points (0,0) and (1,1) belong to one class (output 0), while (0,1) and (1,0) belong to another class (output 1). No single straight line can separate these classes in the input space.\n",
        "\n",
        "### What modifications can be made to the neural network model to handle the XOR gate correctly?\n",
        "\n",
        "  To successfully classify the XOR gate, consider these modifications:\n",
        "  - Use a Multi-Layer Perceptron (MLP): Adding one or more hidden layers with non-linear activation functions (e.g., ReLU or sigmoid) allows the network to learn non-linear boundaries.\n",
        "  - Feature Transformation: Creating new features or using polynomial features could transform the input space into a linearly separable one.\n",
        "  - Increase Neurons in the Hidden Layer: More neurons can help capture the complexity of the XOR function."
      ],
      "metadata": {
        "id": "YzLkXMQu9XJB"
      }
    }
  ]
}